{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Streams Processing\n",
    "\n",
    "- André Bastos, Nº 56969\n",
    "- Carolina Goldstein, Nº 57213\n",
    "- Rafaela Cruz, Nº 56926\n",
    "\n",
    "Run this notebook using the publisher `publisher-debs.sh`, with no additional parameters (i.e., the default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, ceil, concat_ws, window, avg, sum, round\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "    \n",
    "# Longitude and latitude from the upper left corner of the grid, to help conversion\n",
    "init_long = -74.916578\n",
    "init_lat = 41.47718278\n",
    "# Longitude and latitude from the lower right boundaries for filtering purposes\n",
    "limit_long = -73.120778\n",
    "limit_lat = 40.12971598\n",
    "    \n",
    "# Used to filter the rows of the dataset. It returns a boolean value signaling if the row fills all the requirements.\n",
    "# These requirements are that it is not empty, its locations are within the city and that trip time, trip distance\n",
    "# and total amount are above 0\n",
    "def apply_filters(line):\n",
    "    # Split the line by a ,\n",
    "    splitted_line = line.split(',')\n",
    "    # Return boolean\n",
    "    return (\n",
    "        (len(line) > 0) and \\\n",
    "        (float(splitted_line[6]) > init_long) and \\\n",
    "        (float(splitted_line[6]) < limit_long) and \\\n",
    "        (float(splitted_line[7]) > limit_lat) and \\\n",
    "        (float(splitted_line[7]) < init_lat) and \\\n",
    "        (float(splitted_line[8]) > init_long) and \\\n",
    "        (float(splitted_line[8]) < limit_long) and \\\n",
    "        (float(splitted_line[9]) > limit_lat) and \\\n",
    "        (float(splitted_line[9]) < init_lat) and \\\n",
    "        (float(splitted_line[5]) > 0) and \\\n",
    "        (float(splitted_line[4]) > 0) and \\\n",
    "        (float(splitted_line[16]) > 0)\n",
    "        )\n",
    "    \n",
    "\n",
    "# Returns every row as it was with the additional areas based on the locations\n",
    "def get_areas(line, _type = \"bigger\"):\n",
    "    \n",
    "    # Split the line by a ,\n",
    "    splitted_line = line.split(',')\n",
    "    line = splitted_line\n",
    "    \n",
    "    # Longitude and latitude that correspond to a shift in 500 meters\n",
    "    long_shift = 0.005986\n",
    "    lat_shift = 0.004491556\n",
    "\n",
    "    # Longitude and latitude that correspond to a shift in 250 meters\n",
    "    if _type == \"smaller\":\n",
    "        long_shift = long_shift / 2\n",
    "        lat_shift = lat_shift / 2\n",
    "        \n",
    "    return (\n",
    "        line[0], line[1], line[2], line[3], line[4], line[5], line[6],\n",
    "        line[7], line[8], line[9], line[10], line[11], line[12], line[13], line[14], line[15], line[16],\n",
    "        str(math.ceil((float(line[6])-init_long)/long_shift)) + \"-\" + str(math.ceil((init_lat-float(line[7]))/lat_shift)),\n",
    "        str(math.ceil((float(line[8])-init_long)/long_shift)) + \"-\" + str(math.ceil((init_lat-float(line[9]))/lat_shift))\n",
    "        )\n",
    "\n",
    "# Filters locations and some integers for dataframes\n",
    "def filter_locations_and_integers(lines):\n",
    "    split_lines = split(lines[\"value\"], \",\")\n",
    "    lines = lines.filter(split_lines.getItem(6) < limit_long) \\\n",
    "        .filter(split_lines.getItem(6) > init_long) \\\n",
    "        .filter(split_lines.getItem(7) < init_lat) \\\n",
    "        .filter(split_lines.getItem(7) > limit_lat) \\\n",
    "        .filter(split_lines.getItem(8) < limit_long) \\\n",
    "        .filter(split_lines.getItem(8) > init_long) \\\n",
    "        .filter(split_lines.getItem(9) < init_lat) \\\n",
    "        .filter(split_lines.getItem(9) > limit_lat) \\\n",
    "        .filter(split_lines.getItem(5) > 0) \\\n",
    "        .filter(split_lines.getItem(4) > 0) \\\n",
    "        .filter(split_lines.getItem(16) > 0)\n",
    "    return lines\n",
    "\n",
    "# Function that gets the areas from locations\n",
    "def get_areas_df(lines, _type = \"bigger\"):\n",
    "    \n",
    "    # Longitude and latitude that correspond to a shift in 500 meters\n",
    "    long_shift = 0.005986\n",
    "    lat_shift = 0.004491556\n",
    "\n",
    "    # Longitude and latitude that correspond to a shift in 250 meters\n",
    "    if _type == \"smaller\":\n",
    "        long_shift = long_shift / 2\n",
    "        lat_shift = lat_shift / 2\n",
    "        \n",
    "    split_lines = split(lines[\"value\"], \",\")\n",
    "    \n",
    "    lines = lines \\\n",
    "    .withColumn(\"cell_pickup_longitude\", ceil((split_lines.getItem(6).cast(\"double\") - init_long) / long_shift)) \\\n",
    "    .withColumn(\"cell_pickup_latitude\", -ceil((split_lines.getItem(7).cast(\"double\") - init_lat) / lat_shift)) \\\n",
    "    .withColumn(\"cell_dropoff_longitude\", ceil((split_lines.getItem(8).cast(\"double\") - init_long) / long_shift)) \\\n",
    "    .withColumn(\"cell_dropoff_latitude\", -ceil((split_lines.getItem(9).cast(\"double\") - init_lat) / lat_shift))\n",
    "    \n",
    "    lines = lines \\\n",
    "        .withColumn(\"cell_pickup\", concat_ws(\"-\", lines[\"cell_pickup_latitude\"], lines[\"cell_pickup_longitude\"])) \\\n",
    "        .withColumn(\"cell_dropoff\", concat_ws(\"-\", lines[\"cell_dropoff_latitude\"], lines[\"cell_dropoff_longitude\"])) \\\n",
    "        .drop(\"cell_pickup_latitude\", \"cell_pickup_longitude\", \"cell_dropoff_latitude\", \"cell_dropoff_longitude\")\n",
    "    \n",
    "    return lines\n",
    "\n",
    "# Gets the route from the concatenations of both cells\n",
    "def get_routes(lines):\n",
    "    lines = lines.withColumn(\"route\", concat_ws(\"/\", lines[\"cell_pickup\"], lines[\"cell_dropoff\"]))\n",
    "    return lines\n",
    "\n",
    "# Function that does basic pre processing on the data for the dataframes examples\n",
    "def pre_process_df(lines):\n",
    "    # Filter empty rows\n",
    "    lines = lines.na.drop(how=\"all\")\n",
    "    # Filter locations outside current range or bad inputs. Also filter distance time, \n",
    "    # time and total amounts that are less than 0.\n",
    "    lines = filter_locations_and_integers(lines)\n",
    "    return lines\n",
    "\n",
    "# Get only the specified columns\n",
    "# Columns is an array with tuples inside, the tuples got the column name and the respective type\n",
    "def get_columns(lines, columns):\n",
    "    # Array with columns for index\n",
    "    whole_columns = [\"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\", \"trip_time\",\n",
    "                    \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                    \"payment_type\", \"fare_amount\", \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"]\n",
    "    split_lines = split(lines[\"value\"], \",\")\n",
    "    for column in columns:\n",
    "        lines = lines.withColumn(column[0], split_lines.getItem(whole_columns.index(column[0])).cast(column[1]))\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def dumpBatchDF(df, epoch_id):\n",
    "    df.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1 - Spark Streaming Dataframes\n",
    "\n",
    "Find the top 10 most frequent routes during the last 30 minutes:\n",
    "\n",
    "- A route is represented by a starting grid cell and an ending grid cell.\n",
    "- All routes completed within the last 30 minutes are considered for the query.\n",
    "- Use a grid of 300 x 300 cells (each cell is a square of 500 x 500 m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session instance\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka Pstr Project 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"debs\") \\\n",
    "  .load()\n",
    "\n",
    "# Apply filters\n",
    "lines = pre_process_df(lines)\n",
    "\n",
    "# Get specified columns only\n",
    "columns = [(\"dropoff_datetime\", \"timestamp\"), (\"dropoff_longitude\", \"double\"), (\"dropoff_latitude\", \"double\"),\n",
    "          (\"pickup_longitude\", \"double\"), (\"pickup_latitude\", \"double\")]\n",
    "lines = get_columns(lines, columns)\n",
    "\n",
    "# Get the cells from locations\n",
    "lines = get_areas_df(lines)\n",
    "\n",
    "# Join areas to form routes\n",
    "lines = get_routes(lines)\n",
    "\n",
    "# Count the occurrences for each route and present only the top 10 most frequent routes\n",
    "most_frequent_routes = lines.withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "                            .groupBy(window(lines.dropoff_datetime, \"30 minutes\", \"10 minutes\"),\"route\") \\\n",
    "                            .count() \\\n",
    "                            .orderBy(\"window\",\"count\",ascending=False) \\\n",
    "                            .limit(10)\n",
    "\n",
    "query = most_frequent_routes \\\n",
    "            .writeStream \\\n",
    "            .trigger(processingTime=\"10 seconds\") \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .foreachBatch(dumpBatchDF) \\\n",
    "            .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2 - Spark Streaming Dataframes\n",
    "\n",
    "Identify areas that are currently most profitable for taxi drivers:\n",
    "\n",
    "- The profitability of an area is determined by dividing the area profit by the number of dropoffs in that area within the last 15 minutes.\n",
    "- The profit that originates from an area is computed by calculating the average fare + tip for trips that started in the area and ended within the last 15 minutes.\n",
    "- For this problem use a cell size of 250m X 250 m, i.e., a 600 x 600 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session instance\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka Pstr Project 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"debs\") \\\n",
    "  .load()\n",
    "\n",
    "# Apply filters\n",
    "lines = pre_process_df(lines)\n",
    "\n",
    "# Get specified columns only\n",
    "columns = [(\"dropoff_datetime\", \"timestamp\"), (\"pickup_datetime\", \"timestamp\"),\n",
    "    (\"pickup_longitude\", \"double\"), (\"pickup_latitude\", \"double\"),\n",
    "    (\"dropoff_longitude\", \"double\"), (\"dropoff_latitude\", \"double\"), \n",
    "    (\"fare_amount\", \"float\"), (\"tip_amount\", \"float\")]\n",
    "lines = get_columns(lines, columns)\n",
    "\n",
    "# Get the cells from locations\n",
    "lines = get_areas_df(lines, \"smaller\")\n",
    "\n",
    "# Get the profit amount\n",
    "lines = lines.withColumn(\"profit\", lines[\"fare_amount\"] + lines[\"tip_amount\"]) \\\n",
    "             .drop(\"value\") \n",
    "\n",
    "# Select only needed columns\n",
    "lines = lines.select(\"dropoff_datetime\",\"pickup_datetime\", \"profit\", \"cell_pickup\", \"cell_dropoff\")\n",
    "\n",
    "# Compute the average profit by pickup area\n",
    "profit_average = lines.groupBy(lines.cell_pickup,window(lines.dropoff_datetime,\"15 minutes\"))\\\n",
    "                      .agg(avg(lines.profit).alias(\"avg_profit\"))\n",
    "   \n",
    "# Compute total numbers of dropoffs\n",
    "taxis_total = lines.groupBy(lines.cell_dropoff,window(lines.dropoff_datetime,\"30 minutes\"))\\\n",
    "                   .count()\n",
    "\n",
    "                    \n",
    "query = profit_average \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .queryName(\"profit\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\")\\\n",
    "    .start()\n",
    "\n",
    "\n",
    "query2 = taxis_total \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .queryName(\"taxis\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "for i in range(6) :\n",
    "    spark.sql('''select profit.cell_pickup AS cell, profit.window, taxis.window, profit.avg_profit / taxis.count AS profitability \n",
    "    from taxis join profit on \n",
    "    taxis.cell_dropoff = profit.cell_pickup\n",
    "    ORDER BY profit.window, profitability DESC''')\\\n",
    "        .show(5,False)\n",
    "    query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "query2.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3 - Spark Streaming\n",
    "\n",
    "Detect \"slow\" areas:\n",
    "- Compute the average idle time of taxis for each area:\n",
    "    - The idle time of a taxi is the time mediating between the drop off of a ride, and the pickup time of the following ride.\n",
    "    - It is assumed that a taxi is available if it had at least one ride in the last hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime))\n",
    "# If the time between a dropoff and the next pickup is longer than 1 hour, the taxi is not available (is_available = 0)\n",
    "def updateFunctionAvailableTaxis(newValues, runningObj):\n",
    "    if runningObj is None:\n",
    "        runningObj = (newValues[0][0],newValues[0][1],newValues[0][2],newValues[0][3],0)\n",
    "    for v in newValues:\n",
    "        is_available = 1\n",
    "        if datetime.strptime(runningObj[3], \"%Y-%m-%d %H:%M:%S\")-datetime.strptime(v[2], \"%Y-%m-%d %H:%M:%S\") > timedelta(hours=1):\n",
    "            is_available = 0\n",
    "        runningObj = (v[0], v[1], v[2], v[3], is_available)\n",
    "    return runningObj\n",
    "# Output: (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime, is_available))\n",
    "\n",
    "# Input: (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime))\n",
    "# For each ride, append the dropoff area and the dropoff datetime of the last ride\n",
    "def updateFunction(newValues, runningObj):\n",
    "    if runningObj is None:\n",
    "        runningObj = (newValues[0][0],newValues[0][1],newValues[0][2],newValues[0][3],0,0)\n",
    "    else:\n",
    "        for v in newValues:\n",
    "            runningObj = (v[0], v[1], v[2], v[3], runningObj[1], runningObj[3])\n",
    "    return runningObj\n",
    "# Output: (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime, old dropoff area, old dropoff datetime))\n",
    "\n",
    "# Creating the spark context and streaming context objects\n",
    "sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint('checkpoint')\n",
    "lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "            {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "try:\n",
    "    # Kafka sends a timestamp followed by the actual value of the row, so keep the second value in the tuple\n",
    "    filtered_lines = lines.map(lambda line: line[1])\n",
    "    # Apply filters\n",
    "    filtered_lines = filtered_lines.filter(lambda line: apply_filters(line))\n",
    "    # Get areas\n",
    "    lines_areas = filtered_lines.map(lambda line: get_areas(line))\n",
    "    \n",
    "    # We map the needed values to show them in the shape:\n",
    "    # (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime))\n",
    "    datetime_per_taxi = lines_areas.map(lambda line: (line[1],(line[17],line[18],line[2],line[3])))\n",
    "    \n",
    "    # Get only the available taxis - We obtain tuples in the shape:\n",
    "    # (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime, is_available))\n",
    "    # And filter them to keep only those with the parameter is_available = 1\n",
    "    available_taxis = datetime_per_taxi.updateStateByKey(updateFunctionAvailableTaxis) \\\n",
    "                                       .filter(lambda a: a[1][4] == 1) \\\n",
    "                                       .map(lambda a: (a[0],(a[1][0], a[1][1], a[1][2], a[1][3])))\n",
    "    \n",
    "    # For each ride, append the dropoff area and the dropoff datetime of the last ride\n",
    "    # We get the tuples:\n",
    "    # (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime, old dropoff area, old dropoff datetime))\n",
    "    # We filter out the tuples in which the old dropoff time is 0\n",
    "    extended_rides = available_taxis.updateStateByKey(updateFunction) \\\n",
    "                                    .filter(lambda a: a[1][4] != 0)\n",
    "    \n",
    "    # (taxi, (pickup area, dropoff area, pickup datetime, dropoff datetime, old dropoff area, old dropoff datetime))\n",
    "    # New pickup area must be the same as old dropoff area\n",
    "    # Old dropoff datetime must be < than new pickup datetime\n",
    "    processed_extended_rides = extended_rides.filter(lambda a: a[1][0] == a[1][4]) \\\n",
    "                                             .filter(lambda a: datetime.strptime(a[1][5], \"%Y-%m-%d %H:%M:%S\").timestamp() < datetime.strptime(a[1][2], \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "    \n",
    "    # Now we only need the new pickup area, the new pickup datetime and the old dropoff datetime:\n",
    "    # (pickup area, pickup datetime, old dropoff datetime)\n",
    "    # We can then compute the idle time for each ride in each area\n",
    "    idle_time_per_ride_and_area = processed_extended_rides.map(lambda a: (a[1][0], (a[1][2],a[1][5]))) \\\n",
    "                                             .map(lambda a: (a[0], datetime.strptime(a[1][0], \"%Y-%m-%d %H:%M:%S\")-datetime.strptime(a[1][1], \"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "                                             .map(lambda a: (a[0], int(a[1].seconds))) \\\n",
    "                                             .transform(lambda rdd: rdd.sortBy(lambda a: a[1], ascending=False))\n",
    "    \n",
    "    # (pickup area, idle time)\n",
    "    # With the idle time for every ride in each area, we can compute the average idle time for each area\n",
    "    avg_idle_time_per_area = idle_time_per_ride_and_area.map(lambda a: (a[0],(a[1],1))) \\\n",
    "                                                        .reduceByKey(lambda a,b: (a[0]+b[0],a[1]+b[1])) \\\n",
    "                                                        .map(lambda a: (a[0], a[1][0]/a[1][1])) \\\n",
    "                                                        .transform(lambda rdd: rdd.sortBy(lambda a: a[1], ascending=False))\n",
    "\n",
    "    \n",
    "    avg_idle_time_per_area.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination(60)\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4 - Spark Streaming\n",
    "\n",
    "Detect congested areas (routes):\n",
    "- Areas (routes) where, when the taxis enter there, the rides increase in their duration.\n",
    "- For that, there should be alerts when a taxi has a peak in the duration of the ride that is followed by at least 2 rides all increasing in their duration and above area average duration for the last 4 hours.\n",
    "- The alert should contain the location where the taxi started the ride which had the peak duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function that persists through time the information of the number of rides that rise sequentially in\n",
    "# duration for all routes\n",
    "# Example of newValues (120.0, '-73.989525', '40.741528')\n",
    "# This updateState does not need artificial window because it already resets values in it's nature\n",
    "def updateFunction(newValues, runningObj):\n",
    "    if runningObj is None:\n",
    "        runningObj = (newValues[0][0], 0, 0, newValues[0][1], newValues[0][2], newValues[0][0])\n",
    "    for v in newValues:\n",
    "        # counter - counts the number of sequential rides\n",
    "        counter = runningObj[1]\n",
    "        # prev_dur - previous rows duration\n",
    "        prev_dur = runningObj[0]\n",
    "        # dur - target's row duration\n",
    "        dur = v[0]\n",
    "        # long - target's row long\n",
    "        long = runningObj[2]\n",
    "        # lat - target's row lat\n",
    "        lat = runningObj[3]\n",
    "        # first_dur - first ride's duration of the sequence of rides\n",
    "        first_dur = runningObj[4]\n",
    "\n",
    "        # if current duration is inferior to the previous one, reset counter\n",
    "        if dur <= prev_dur:\n",
    "            counter = 0\n",
    "        else: counter += 1\n",
    "\n",
    "        # if reset happens, also reset some variables to persist from first ride\n",
    "        if counter == 0:\n",
    "            long = v[1]\n",
    "            lat = v[2]\n",
    "            first_dur = v[0]\n",
    "\n",
    "        runningObj = (dur, counter, long, lat, first_dur)\n",
    "    return runningObj\n",
    "\n",
    "# Update function that gets the count of duration per route\n",
    "# Example of newValues (120.0, pickup_datetime, dropoff_datetime)\n",
    "# Also has an artificial window that resets computations when 4 hours have passed\n",
    "def updateFunctionMean(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = (0.0, 0.0, newValues[0][1], newValues[0][2])\n",
    "    for v in newValues:\n",
    "\n",
    "        runningCount = (v[0] + runningCount[0], runningCount[1]+1, runningCount[2], v[2])\n",
    "\n",
    "        if datetime.strptime(v[2], \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(runningCount[2], \"%Y-%m-%d %H:%M:%S\") > timedelta(hours=4):\n",
    "            runningCount = (v[0], 1, v[1], v[2])\n",
    "\n",
    "    return runningCount\n",
    "\n",
    "# Creating the spark context and streaming context objects\n",
    "sc = SparkContext(\"local[2]\", \"KafkaExample\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint('checkpoint')\n",
    "lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
    "            {\"metadata.broker.list\": \"kafka:9092\"})\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Kafka sends a timestamp followed by the actual value of the row, so keep the second value in the tuple\n",
    "    filtered_lines = lines.map(lambda line: line[1])\n",
    "    # Apply filters\n",
    "    filtered_lines = filtered_lines.filter(lambda line: apply_filters(line))\n",
    "    # Get areas\n",
    "    lines_areas = filtered_lines.map(lambda line: get_areas(line))\n",
    "\n",
    "    # Get desired columns only\n",
    "    desired_columns = lines_areas.map(lambda line: (str(line[17]) + \":\" + str(line[18]), (float(line[4]), line[6], line[7])))\n",
    "\n",
    "    # Get rows with number of rising rides through the beginning\n",
    "    counters = desired_columns.updateStateByKey(updateFunction)\n",
    "\n",
    "    # Only get the rows that had a sequential ride number equal or above to 2\n",
    "    # Example: ('315-325:379-372', (560.0, 1, 0, '-73.97625', '40.748528', 234,1))\n",
    "    counters_filtered = counters.filter(lambda line: line[1][1] >= 2)\n",
    "\n",
    "    # Get desired columns for mean computation\n",
    "    desired_columns_means = lines_areas.map(lambda line: (str(line[17]) + \":\" + str(line[18]), (float(line[4]), line[2], line[3])))\n",
    "\n",
    "    # Get the mean of duration per route\n",
    "    means = desired_columns_means.updateStateByKey(updateFunctionMean) \\\n",
    "            .map(lambda line: (line[0], float(line[1][0]) / float(line[1][1]), line[1][2], line[1][3]))\n",
    "\n",
    "    # Joins the two above together and flattens, took summed duration\n",
    "    joined = counters_filtered.join(means) \\\n",
    "            .map(lambda line: (line[0], line[1][0][1], line[1][0][2], line[1][0][3], line[1][0][4], line[1][1]))\n",
    "\n",
    "    # Per position meanings: (area, nr_rides, first_long, first_lat, first_dur, area_mean_dur)\n",
    "    # Filter out rows that have a first ride duration equal or less than the mean of the same route\n",
    "    peak_filter = joined.filter(lambda line: float(line[4]) > float(line[5]))\n",
    "    \n",
    "    # Get only useful information to show\n",
    "    peak_filter = peak_filter.map(lambda line: (line[0], line[1], line[2], line[3]))\n",
    "\n",
    "    peak_filter.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination(60)\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 5 - Spark Streaming Dataframes\n",
    "\n",
    "Select the most pleasant taxi drivers:\n",
    "\n",
    "- To distinguish the most pleasant taxi driver in one day, it should be shown the taxi driver with the highest percentage of tips in that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session instance\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka Pstr Project 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"debs\") \\\n",
    "  .load()\n",
    "\n",
    "# Apply filters\n",
    "lines = pre_process_df(lines)\n",
    "\n",
    "# Get specified columns only\n",
    "columns = [(\"dropoff_datetime\", \"timestamp\"), (\"hack_license\", \"string\"), (\"tip_amount\", \"double\"), \n",
    "          (\"total_amount\", \"double\")]\n",
    "lines = get_columns(lines, columns)\n",
    "\n",
    "# Get tip percentage\n",
    "lines = lines.withColumn(\"tip_percentage\", lines[\"tip_amount\"]/lines[\"total_amount\"]) \\\n",
    "    .drop(\"value\",\"key\",\"topic\",\"partition\",\"offset\",\"timestamp\",\"timestampType\")\n",
    "\n",
    "# For each taxi driver, compute the average tip percentage and show only the taxi driver with the highest value\n",
    "# for this average\n",
    "avg_tip_percentage = lines.groupBy(window(\"dropoff_datetime\",\"1 days\"),\"hack_license\") \\\n",
    "                          .avg(\"tip_percentage\") \\\n",
    "                          .orderBy('avg(tip_percentage)',ascending=False) \\\n",
    "                          .limit(1)\n",
    "\n",
    "query = avg_tip_percentage \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .trigger(processingTime=\"60 seconds\") \\\n",
    "            .foreachBatch(dumpBatchDF) \\\n",
    "            .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6 - Spark Streaming Dataframes\n",
    "\n",
    "What does the price discrimination look like for the pickup areas where the taxi drivers make most money, in each 2 hour window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session instance\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka Pstr Project 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"debs\") \\\n",
    "  .load()\n",
    "\n",
    "# Apply filters\n",
    "lines = pre_process_df(lines)\n",
    "\n",
    "# Get specified columns only\n",
    "columns = [(\"pickup_datetime\", \"timestamp\"), (\"fare_amount\", \"float\"), (\"surcharge\", \"float\"), \n",
    "          (\"mta_tax\", \"float\"), (\"tip_amount\", \"float\"), (\"tolls_amount\", \"float\"), (\"total_amount\", \"float\")]\n",
    "lines = get_columns(lines, columns)\n",
    "\n",
    "# Get the cells from locations\n",
    "lines = get_areas_df(lines)\n",
    "\n",
    "lines = lines.drop(\"value\",\"key\",\"topic\",\"partition\",\"offset\",\"timestamp\",\"timestampType\",\"cell_pickup_latitude\", \"cell_pickup_longitude\")\n",
    "\n",
    "# For each pickup area and each time interval of 2 hours, we compute the sum of the total amount and, of that\n",
    "# total amount, what percentage corresponds to the fare amount, the surcharge, the mta_tax, the tip_amount \n",
    "# and the toll_amount\n",
    "# We order by the summed total amount, to show these statistics only for the areas where taxi drivers make\n",
    "# more money\n",
    "statistics = lines.groupBy(\"cell_pickup\", window(\"pickup_datetime\",\"2 hours\")) \\\n",
    "                  .agg(round(sum(\"total_amount\"),3).alias(\"sum_total_amount\"), \\\n",
    "                       round(sum(\"fare_amount\")/sum(\"total_amount\"),3).alias(\"fare\"), \\\n",
    "                       round(sum(\"surcharge\")/sum(\"total_amount\"),3).alias(\"surcharge\"),\\\n",
    "                       round(sum(\"mta_tax\")/sum(\"total_amount\"),3).alias(\"mta_tax\"),\\\n",
    "                       round(sum(\"tip_amount\")/sum(\"total_amount\"),3).alias(\"tip\"),\\\n",
    "                       round(sum(\"tolls_amount\")/sum(\"total_amount\")).alias(\"tolls\"))\\\n",
    "                  .orderBy(\"sum_total_amount\", ascending=False) \\\n",
    "                  .limit(5)\n",
    "\n",
    "query = statistics \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .trigger(processingTime=\"30 seconds\") \\\n",
    "            .foreachBatch(dumpBatchDF) \\\n",
    "            .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 7 - Spark Streaming Dataframes\n",
    "\n",
    "In which routes the total amount per mile is highest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session instance\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Kafka Pstr Project 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"debs\") \\\n",
    "  .load()\n",
    "\n",
    "# Apply filters\n",
    "lines = pre_process_df(lines)\n",
    "\n",
    "# Get specified columns only\n",
    "columns = [(\"pickup_datetime\", \"timestamp\"), (\"pickup_longitude\", \"double\"), (\"pickup_latitude\", \"double\"),\n",
    "            (\"dropoff_longitude\", \"double\"), (\"dropoff_latitude\", \"double\"),\n",
    "            (\"trip_distance\", \"float\"), (\"total_amount\", \"float\")]\n",
    "lines = get_columns(lines, columns)\n",
    "\n",
    "# Get the cells from locations\n",
    "lines = get_areas_df(lines)\n",
    "\n",
    "# Join areas to form routes\n",
    "lines = get_routes(lines)\n",
    "\n",
    "lines = lines.drop(\"value\",\"key\",\"topic\",\"partition\",\"offset\",\"timestamp\",\"timestampType\")\n",
    "\n",
    "# For each hour and each route, we compute the summed total amount and the summed trip distance \n",
    "total_per_route = lines.groupBy(window(\"pickup_datetime\", \"1 hours\"),\"route\") \\\n",
    "                       .agg(sum(\"total_amount\").alias(\"sum_total_amount\"),\n",
    "                            sum(\"trip_distance\").alias(\"total_distance\"))\n",
    "                        \n",
    "# We add a new column to the dataframe with the division between the summed total amount and the \n",
    "# summed trip distance\n",
    "profit_per_route = total_per_route.withColumn(\"amount_per_mile\", \n",
    "                                               total_per_route[\"sum_total_amount\"]/total_per_route[\"total_distance\"]) \\\n",
    "                                  .drop(\"sum_total_amount\",\"total_distance\")\n",
    "\n",
    "# We order the results by the total amount per mile, the obtain the top 10 routes in which\n",
    "# a taxi driver can make more money per mile\n",
    "most_profitable_routes = profit_per_route.orderBy(\"amount_per_mile\", ascending=False) \\\n",
    "                                         .limit(10)\n",
    "\n",
    "query = most_profitable_routes \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .trigger(processingTime=\"5 seconds\") \\\n",
    "            .foreachBatch(dumpBatchDF) \\\n",
    "            .start()\n",
    "\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
